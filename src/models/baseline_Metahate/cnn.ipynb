{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bdce7130ac6627e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# CNN baseline for MetaHate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Flatten\n",
    "from keras.layers import TextVectorization\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dfc8963507f8ea",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b464897186e2ad59",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../../data/processed/within-dataset/metahate_train.tsv', sep='\\t', names=['label', 'text'],header=0)\n",
    "data = data.dropna(subset=['text'])\n",
    "\n",
    "data['text'] = data['text'].astype(str)\n",
    "\n",
    "texts_train = data['text'].tolist()\n",
    "labels_train = data['label'].tolist()\n",
    "\n",
    "data = pd.read_csv('../../../data/processed/within-dataset/metahate_test.tsv', sep='\\t', names=['label', 'text'],header=0)\n",
    "\n",
    "data = data.dropna(subset=['text'])\n",
    "\n",
    "data['text'] = data['text'].astype(str)\n",
    "\n",
    "texts_test = data['text'].tolist()\n",
    "labels_test = data['label'].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2158adce",
   "metadata": {},
   "source": [
    "## Standardize labels all to string and enerate validation set from train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa5718f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_train_noVal, texts_val, labels_train_noVal, labels_val = train_test_split(\n",
    "    texts_train,\n",
    "    labels_train,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3f2ef29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "labels_train.count('1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa36005d643cead",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Tokenizing the text data and convert to sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1734826f265f6e05",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_tokens = 10000\n",
    "sequence_length = 512\n",
    "\n",
    "vectorizer = TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length\n",
    ")\n",
    "labels_train_noVal = np.array(labels_train_noVal, dtype=np.float32)\n",
    "labels_val = np.array(labels_val, dtype=np.float32)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (texts_train_noVal, labels_train_noVal)\n",
    ").batch(32)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (texts_val, labels_val)\n",
    ").batch(32)\n",
    "vectorizer.adapt(texts_train_noVal)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b279bd4082ef15",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Creating a simple neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58774c9d206c1c35",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating a Sequential model\n",
    "model = Sequential([\n",
    "    vectorizer,  # <-- replaces Tokenizer + pad_sequences   \n",
    "    Embedding(\n",
    "        input_dim=vectorizer.vocabulary_size(),\n",
    "        output_dim=64\n",
    "    ),\n",
    "    Flatten(),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3600272d95a09dba",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "## Compiling and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "634fd225a8b1c842",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ text_vectorization_1            │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TextVectorization</span>)             │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ text_vectorization_1            │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mTextVectorization\u001b[0m)             │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer='adam', # 'adam' is chosen as the optimization algorithm, known for its efficiency in training neural networks\n",
    "    loss='binary_crossentropy', # 'binary_crossentropy' is selected as the loss function as we are performing a binary classification tasks\n",
    "    metrics= ['accuracy'] # The model will be evaluated based on 'accuracy' during training\n",
    ")\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a5c38a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m22024/22024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m886s\u001b[0m 40ms/step - accuracy: 0.8523 - loss: 0.3320 - val_accuracy: 0.8619 - val_loss: 0.3137\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x20fe48fa010>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds, validation_data=val_ds, epochs=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1526f38ca177aa91",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Evaluating the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed582d11591e3808",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  86/6883\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:11\u001b[0m 10ms/step"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6883/6883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 8ms/step\n"
     ]
    }
   ],
   "source": [
    "# Obtaining raw predictions for the test set by thresholding the predictions at 0.5 and converting boolean values to integers (0 or 1)\n",
    "\n",
    "predict_ds = tf.data.Dataset.from_tensor_slices(texts_test).batch(32)\n",
    "predictions = (model.predict(predict_ds) > 0.5).astype(int).flatten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edeb6c5a4ca79166",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Calculating the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b61373fe2b3fa2f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8623594102609509\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.94      0.91    173537\n",
      "           1       0.72      0.58      0.64     46696\n",
      "\n",
      "    accuracy                           0.86    220233\n",
      "   macro avg       0.80      0.76      0.78    220233\n",
      "weighted avg       0.86      0.86      0.86    220233\n",
      "\n",
      "Weighted F1 Score: 0.8569224678167362\n",
      "Micro F1 Score: 0.8623594102609509\n",
      "Macro F1 Score: 0.7782966307894152\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(labels_test, predictions)\n",
    "report = classification_report(labels_test, predictions)\n",
    "weighted_f1 = f1_score(labels_test, predictions, average='weighted')\n",
    "micro_f1 = f1_score(labels_test, predictions, average='micro')\n",
    "macro_f1 = f1_score(labels_test, predictions, average='macro')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\\n\", report)\n",
    "print(f\"Weighted F1 Score: {weighted_f1}\")\n",
    "print(f\"Micro F1 Score: {micro_f1}\")\n",
    "print(f\"Macro F1 Score: {macro_f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
